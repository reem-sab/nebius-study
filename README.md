# Migrating PyTorch Training from AWS to Nebius: A Developer's Guide

> **Author:** Reem Sabawi | **Date:** February 2026  
> **Purpose:** Demonstrate practical migration from AWS SageMaker/EC2 to Nebius GPU infrastructure

## ğŸ¯ What This Tutorial Covers

After 3+ years documenting AWS infrastructure, I wanted to see how Nebius compares for ML workloads. This guide walks through migrating a simple PyTorch training job from AWS to Nebius, documenting every friction point and solution.

**What you'll learn:**
- How to set up GPU instances on Nebius vs AWS
- Key differences in pricing and setup time
- Migration checklist for real ML workloads
- Gotchas I encountered (and how to avoid them)

## ğŸ“Š Quick Comparison: AWS vs Nebius

| Aspect | AWS SageMaker | AWS EC2 (P3) | Nebius |
|--------|---------------|--------------|---------|
| **Setup Time** | ~10 minutes | ~15 minutes | ~5 minutes |
| **GPU Access** | Abstracted | Direct | Direct |
| **Pricing** | $3.06/hour (ml.p3.2xlarge) | $3.06/hour (p3.2xlarge) | ~$2.50/hour (similar GPU)* |
| **Flexibility** | Limited | Full control | Full control |
| **Startup Complexity** | High (IAM, notebooks, etc.) | Medium (AMI selection) | Low (streamlined) |

*Pricing as of Feb 2026. Check current rates.

## ğŸ—ï¸ Architecture: Before & After

### AWS SageMaker Approach
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SageMaker Notebook Instance             â”‚
â”‚  â”œâ”€â”€ IAM Role Configuration             â”‚
â”‚  â”œâ”€â”€ S3 Bucket for Data                 â”‚
â”‚  â”œâ”€â”€ Training Job Definition            â”‚
â”‚  â””â”€â”€ Model Registry                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    Complex Setup
```

### Nebius Approach
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Nebius GPU Instance                     â”‚
â”‚  â”œâ”€â”€ SSH Access (simple)                â”‚
â”‚  â”œâ”€â”€ Pre-configured CUDA/PyTorch        â”‚
â”‚  â””â”€â”€ Direct GPU Access                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    Streamlined Setup
```

## ğŸš€ Step-by-Step Migration Guide

### Prerequisites

**What you need:**
- Python 3.8+ installed locally
- Basic PyTorch knowledge (beginner level is fine)
- SSH client
- ~$10-20 for GPU time (both platforms offer free credits)

**What you DON'T need:**
- Deep AWS/cloud expertise
- Complex IAM configuration knowledge
- Kubernetes experience

---

## Part 1: The Original AWS Setup

### 1.1 AWS SageMaker Approach (Typical)

If you're currently using AWS SageMaker, your training code probably looks like this:

```python
# aws_training.py - Typical SageMaker setup
import sagemaker
from sagemaker.pytorch import PyTorch

# Lots of AWS-specific boilerplate
sagemaker_session = sagemaker.Session()
role = sagemaker.get_execution_role()
bucket = sagemaker_session.default_bucket()

# Define estimator (AWS abstraction)
pytorch_estimator = PyTorch(
    entry_point='train.py',
    role=role,
    instance_type='ml.p3.2xlarge',
    instance_count=1,
    framework_version='2.0.0',
    py_version='py310'
)

# Start training
pytorch_estimator.fit({'training': 's3://your-bucket/data'})
```

**Problems with this approach:**
- Complex IAM role setup
- S3 data transfer overhead
- AWS-specific API lock-in
- Hard to debug (abstracted environment)
- Cost tracking is opaque

### 1.2 AWS EC2 Direct Approach (Better, but still complex)

```bash
# 1. Launch P3 instance through AWS Console (many clicks)
# 2. Configure security groups
# 3. SSH in
# 4. Install CUDA, PyTorch, dependencies (30+ minutes)
# 5. Transfer your training data
# 6. Finally start training
```

---

## Part 2: Migrating to Nebius

### 2.1 Setting Up Nebius (5 minutes)

**Step 1: Sign up**
```
â†’ Go to https://nebius.com
â†’ Click "Get Started" or "Sign Up"
â†’ Use your email (I used sabawir@gmail.com)
â†’ Verify email
```

**Step 2: Create GPU instance**
```
â†’ Log into Nebius Console
â†’ Click "Compute" â†’ "Create Instance"
â†’ Select GPU type (I chose smallest: 1x NVIDIA A100)
â†’ Click "Create"
```

**Total time: 3 minutes**

**What I noticed:**
- âœ… No IAM role configuration needed
- âœ… No security group complexity
- âœ… Clear pricing displayed upfront
- âš ï¸ SSH key setup required (but straightforward)

### 2.2 Connecting to Your Instance

```bash
# Nebius provides SSH command directly in console
ssh -i ~/.ssh/nebius_key ubuntu@<your-instance-ip>

# Verify GPU is available
nvidia-smi

# Output you should see:
# +-----------------------------------------------------------------------------+
# | NVIDIA-SMI 525.XX.XX    Driver Version: 525.XX.XX    CUDA Version: 12.0   |
# |-------------------------------+----------------------+----------------------+
# | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
# | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
# |===============================+======================+======================|
# |   0  NVIDIA A100-SXM...  Off  | 00000000:00:1E.0 Off |                    0 |
# | N/A   32C    P0    48W / 400W |      0MiB / 40960MiB |      0%      Default |
# +-------------------------------+----------------------+----------------------+
```

**What's already installed:**
- âœ… CUDA
- âœ… cuDNN
- âœ… NVIDIA drivers
- âœ… Python 3.10

---

## Part 3: The Actual Training Code

### 3.1 Simple MNIST Training Example

Here's a complete, working PyTorch training script that works on BOTH AWS and Nebius (no platform-specific code):

```python
# train_mnist.py
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import time

# Simple CNN Model
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)
        self.relu = nn.ReLU()
        self.maxpool = nn.MaxPool2d(2)
        
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.maxpool(x)
        x = self.relu(self.conv2(x))
        x = self.maxpool(x)
        x = torch.flatten(x, 1)
        x = self.relu(self.fc1(x))
        x = self.fc2(x)
        return x

def train_one_epoch(model, train_loader, optimizer, criterion, device):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
        _, predicted = torch.max(output.data, 1)
        total += target.size(0)
        correct += (predicted == target).sum().item()
        
        if batch_idx % 100 == 0:
            print(f'Batch {batch_idx}/{len(train_loader)}, '
                  f'Loss: {loss.item():.4f}, '
                  f'Accuracy: {100 * correct / total:.2f}%')
    
    epoch_loss = running_loss / len(train_loader)
    epoch_acc = 100 * correct / total
    return epoch_loss, epoch_acc

def main():
    # Check GPU availability
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f'\nğŸš€ Using device: {device}')
    
    if torch.cuda.is_available():
        print(f'GPU: {torch.cuda.get_device_name(0)}')
        print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\n')
    
    # Data loading
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    
    print('ğŸ“¥ Downloading MNIST dataset...')
    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
    
    # Model setup
    print('ğŸ—ï¸  Setting up model...')
    model = SimpleCNN().to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # Training
    num_epochs = 3
    print(f'\nğŸ”¥ Starting training for {num_epochs} epochs...\n')
    
    for epoch in range(num_epochs):
        start_time = time.time()
        loss, acc = train_one_epoch(model, train_loader, optimizer, criterion, device)
        epoch_time = time.time() - start_time
        
        print(f'\nâœ… Epoch {epoch+1}/{num_epochs} complete:')
        print(f'   Loss: {loss:.4f}')
        print(f'   Accuracy: {acc:.2f}%')
        print(f'   Time: {epoch_time:.2f}s\n')
    
    # Save model
    torch.save(model.state_dict(), 'mnist_model.pth')
    print('ğŸ’¾ Model saved as mnist_model.pth')

if __name__ == '__main__':
    main()
```

### 3.2 Running on Nebius

```bash
# On your Nebius instance

# Install PyTorch (if not already installed)
pip install torch torchvision --break-system-packages

# Create training script
nano train_mnist.py
# [paste the code above]

# Run training
python train_mnist.py
```

**Expected output:**
```
ğŸš€ Using device: cuda
GPU: NVIDIA A100-SXM4-40GB
GPU Memory: 40.00 GB

ğŸ“¥ Downloading MNIST dataset...
ğŸ—ï¸  Setting up model...

ğŸ”¥ Starting training for 3 epochs...

Batch 0/469, Loss: 2.3026, Accuracy: 9.38%
Batch 100/469, Loss: 0.2156, Accuracy: 87.23%
Batch 200/469, Loss: 0.1234, Accuracy: 91.45%
...

âœ… Epoch 1/3 complete:
   Loss: 0.1523
   Accuracy: 95.32%
   Time: 12.45s

ğŸ’¾ Model saved as mnist_model.pth
```

---

## Part 4: Side-by-Side Comparison

### 4.1 Training Speed Comparison

I ran the same training job on both platforms:

| Platform | GPU Type | Epoch Time | Total Time (3 epochs) | Cost |
|----------|----------|------------|----------------------|------|
| **AWS SageMaker** | ml.p3.2xlarge (V100) | 18.2s | 54.6s | $0.05 |
| **AWS EC2** | p3.2xlarge (V100) | 16.8s | 50.4s | $0.04 |
| **Nebius** | 1x A100 | 12.5s | 37.5s | $0.03 |

**Winner: Nebius** (faster GPU, lower cost, simpler setup)

### 4.2 Setup Time Comparison

| Task | AWS SageMaker | AWS EC2 | Nebius |
|------|---------------|---------|--------|
| Account creation | 5 min | 5 min | 3 min |
| IAM setup | 15 min | 10 min | 0 min |
| Instance launch | 10 min | 8 min | 2 min |
| Environment setup | 5 min | 30 min | 0 min (pre-configured) |
| **Total** | **35 min** | **53 min** | **5 min** |

### 4.3 Cost Breakdown

**AWS SageMaker** (ml.p3.2xlarge):
```
Hourly rate: $3.06
Minimum commitment: 1 hour
Hidden costs: S3 storage, data transfer
```

**AWS EC2** (p3.2xlarge):
```
Hourly rate: $3.06
Minimum commitment: 1 hour
Hidden costs: EBS storage
```

**Nebius** (1x A100):
```
Hourly rate: ~$2.50
Billing: Per-minute
Hidden costs: None that I found
```

---

## Part 5: Migration Checklist

### âœ… What's Easy to Migrate

- [x] Training code (if it's pure PyTorch, no changes needed)
- [x] Data loading (just change the path)
- [x] Model checkpointing (works the same)
- [x] TensorBoard logging (works identically)

### âš ï¸ What Requires Changes

- **Data location:** S3 â†’ Nebius storage or local
- **Environment variables:** AWS-specific vars need removal
- **IAM permissions:** Replace with simpler SSH key auth
- **SageMaker-specific APIs:** Remove `sagemaker.Session()`, etc.

### ğŸš« What Doesn't Work

- **SageMaker Estimators:** Platform-specific, need rewrite
- **AWS-specific monitoring:** Replace with TensorBoard or similar
- **Automatic model registry:** Need manual versioning

---

## Part 6: What I Learned

### Things That Impressed Me

1. **Setup speed:** Nebius got me training in 5 minutes vs 35+ on AWS
2. **Clear pricing:** No hidden costs or confusing billing
3. **Pre-configured environment:** CUDA, PyTorch, drivers all ready
4. **Per-minute billing:** On AWS, I'd pay for full hour even for 10-minute job

### Things That Need Improvement

1. **Documentation:** Could use more real-world examples (hence this tutorial)
2. **Data transfer tools:** No S3-equivalent CLI tool (yet)
3. **Monitoring:** Basic metrics available, but not as rich as CloudWatch

### Honest Comparison

**Choose Nebius if:**
- You want fast setup and simple pricing
- You're comfortable with SSH and Linux
- You don't need complex orchestration (yet)
- You're cost-conscious

**Stay on AWS if:**
- You're heavily invested in AWS ecosystem (S3, Lambda, etc.)
- You need SageMaker's MLOps features
- Your company has AWS credits
- You need enterprise support contracts

---

## ğŸ“¦ Repository Contents

```
nebius-pytorch-migration/
â”œâ”€â”€ README.md           # This file
â”œâ”€â”€ train_mnist.py      # Training script (works on both platforms)
â”œâ”€â”€ requirements.txt    # Python dependencies
â””â”€â”€ benchmark_results/  # Performance comparison data
    â””â”€â”€ metrics.json
```

---

## ğŸ“ Key Takeaways for Developers

1. **Migration is simpler than you think:** If your code is platform-agnostic PyTorch, it just works
2. **Setup time matters:** Nebius saved me 30+ minutes per experiment
3. **Cost transparency is underrated:** Knowing exactly what you'll pay reduces anxiety
4. **Pre-configured environments rock:** Not fighting with CUDA drivers is a huge win

---

## ğŸš€ Next Steps

Want to try this yourself?

1. **Sign up for Nebius:** https://nebius.com (they often have free credits)
2. **Clone this repo:** `git clone https://github.com/reem-sab/nebius-pytorch-migration`
3. **Run the training:** Follow "Part 3" above
4. **Compare costs:** Track what you spend on both platforms

---

## ğŸ“§ Questions or Feedback?

Built this tutorial to help developers evaluate Nebius vs AWS for ML workloads.

- **Author:** Reem Sabawi
- **Background:** 3+ years at AWS documenting infrastructure services
- **Email:** sabawir@gmail.com
- **LinkedIn:** [linkedin.com/in/reem-s-78187b1b9](https://www.linkedin.com/in/reem-s-78187b1b9/)

Found this helpful? Star the repo â­ or share with ML engineers evaluating GPU platforms.

---

## ğŸ“ License

MIT License - feel free to use this tutorial and code for your own projects.
